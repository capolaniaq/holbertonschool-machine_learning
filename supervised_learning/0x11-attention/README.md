# 0x11. Attention

## Author: Carlos Andres Polania (capolaniaq@correo.udistrital.edu.co)

-   What is the attention mechanism?
-   How to apply attention to RNNs
-   What is a transformer?
-   How to create an encoder-decoder transformer model
-   What is GPT?
-   What is BERT?
-   What is self-supervised learning?
-   How to use BERT for specific NLP tasks
-   What is SQuAD? GLUE?
## Tasks

### 0. RNN Encoder

### 1. Self Attention

### 2. RNN Decoder

### 3. Positional Encoding

### 4. Scaled Dot Product Attention

### 5. Multi Head Attention

### 6. Transformer Encoder Block

### 7. Transformer Decoder Block

### 8. Transformer Encoder
### 9. Transformer Decoder

### 10. Transformer Network


